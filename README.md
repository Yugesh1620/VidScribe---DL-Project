# VidScribe---DL-Project
In an era dominated by video content, the accessibility and
discoverability of videos are often hindered by insufficient
descriptive captions. This paper presents an approach to ad-
dressing this challenge by leveraging deep learning models
to develop an efficient system for generating semantically
accurate and contextually rich video descriptions. Using the
MSR-VTT dataset as the benchmark, our work explores state-
of-the-art models, including CLIP, BART and T5 to identify
the optimal solution for video understanding, caption retrieval
and caption generation. Our findings demonstrate significant
advancements in video analysis, particularly in overcoming
the computational constraints of existing multimodal models
for video understanding. This work tries to enhance video
accessibility and contextualization across diverse domains,
including education, entertainment, and marketing

# Contributors
- Yugesh Panta
- Sachin Kunta
- Revin Dsilva
